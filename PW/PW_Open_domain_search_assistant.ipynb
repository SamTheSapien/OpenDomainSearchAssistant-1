{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PW_Open_domain_search_assistant.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"B-ohecYhwESO"},"source":["#Open_domain_search_assistant\n","\n","50483 - Joao Diogo\n","\n","\n","53044 - Gonçalo Antunes\n","\n","\n","53600 - Samuel Viegas"]},{"cell_type":"markdown","metadata":{"id":"xm3eZD11xLkH"},"source":["###Preparar workspace, modelos e dependências"]},{"cell_type":"code","metadata":{"id":"6efOvVbjWGMe"},"source":["# Colab Setup\n","# Mount your Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# After downloading the shared starting point folder as a Zip\n","# Unzip it and re-upload it to a location on your GDrive\n","\n","# This command copies the contents from the folder you uploaded to GDrive, to the colab working dir\n","!cp -r /content/drive/MyDrive/PW /content\n","\n","# Add working dir to the sys path, so that we can find the aux python files when running the Notebook\n","import sys\n","if not '/content/ProjectoPW2021' in sys.path:\n","  sys.path += ['/content/ProjectoPW2021']\n","\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import urllib\n","from scipy.sparse import csr_matrix\n","from time import time\n","!pip install scikit-network\n","#from sknetwork.visualization import svg_graph, svg_digraph, svg_bigraph\n","from sklearn.decomposition import randomized_svd\n","from IPython.display import SVG\n","from sknetwork.visualization import svg_graph\n","import pprint\n","\n","#import wikipedia_principal_eigenvector as pagerank\n","\n","#import the BLINK/DBpedia-spotlight\n","#because DBpedia-spotlight is REST based we need to import urls and network librarys to make requests\n","import requests\n","!pip install transformers\n","from transformers import BartModel, BartConfig, pipeline\n","# Initializing a BART facebook/bart-large style configuration\n","configuration = BartConfig()\n","# Initializing a model from the facebook/bart-large style configuration\n","model = BartModel(configuration)\n","# Accessing the model configuration\n","configuration = model.config\n","summarization = pipeline(\"summarization\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vATb-VKyaZEC"},"source":["## 1º passo\n","\n","1. Preparar o json (`allin.json`) contendo todas as conversas por processar\n","\n","2. Utilizar o trabalho de RI juntamente com o elasticSearch para ter o Groundtruth File com perguntas e respetivas 10 melhores respostas\n","\n","</br>\n","\n","Objeto: `conversa`\n","\n","</br>\n","\n","---\n","\n","## 2º passo\n","\n","+ Qual o entity linker a usar de modo a extrair entidades? \n","\n","  - [DBpedia-spotlight](https://www.dbpedia-spotlight.org/api)\n","\n","+ Outras hipóteses:\n","\n","  - [Blinker](https://github.com/facebookresearch/BLINK)\n","\n","\n","</br>\n","\n","<ins>Nota:</ins> Inicialmente os resultados não eram `case sensitive`\n","\n","Exemplo:\n","\n","| Entidades |1|2|3|4| ... |\n","|----------:|----------:|----------:|----------:| ----------:|----------:|\n","| `Tópico 1` |Species| species | tiger shark | bull shark | (...)|\n","\n","</br>\n","\n","Objeto: `allMatrixAllDataset`\n","\n","</br>\n","\n","---\n","\n","## 3º passo\n","\n","Aplicar ao grafo das entidades o algoritmo Pagerank dado nas aulas\n","[entitys, entitys]-> Pagerank -> [p_results,n]\n","\n","Pagerank: https://scikit-learn.org/stable/auto_examples/applications/wikipedia_principal_eigenvector.html\n","\n","TO-DO:\n","\n","Thresholds a calibrar que não estão na tabela:\n","\n","1-Na criação do grafo para remover entidades\n","\n","2-No stopping criteria do PR (não é preciso)\n","\n","3-Peso das entidades das Queries vs das passagens\n","\n","Modos de funcionamento:\n","\n","1-Entities of the passages of the current turn (DONE)\n","\n","2-Entities of the the passages of the first turn (DONE)\n","\n","3-Entities of the passages returned in previous turns (DONE)\n","\n","4-Q entities penalty"]},{"cell_type":"markdown","metadata":{"id":"6r87ehQSQ0Gv"},"source":["## FLAGS"]},{"cell_type":"code","metadata":{"id":"KSQi146ZQxdg"},"source":["# false -> Matrix value's are 1s\n","# true  -> Matrix value's ranged 0.5 - 1.0\n","MATRIX_FRAC = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4nPFfzBQRLRV"},"source":["# Our Python code\n","## Loading pickles, dbpedia-entity-retrieval, pageRank\n","\n","!cp '/content/PW/Python/SearchAssistant.py' .\n","!cp '/content/PW/Python/AssistantTools.py' .\n","\n","import SearchAssistant\n","import AssistantTools\n","\n","# Variables\n","## Our object, complete topics, complete matrices\n","assistant = SearchAssistant.getAssistant(MATRIX_FRAC)\n","\n","conversa = assistant.getConversa()\n","\n","allMatrixAllDataset = assistant.getAllMatrixAllDataset()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eJg2E4_1i2Ks"},"source":["As seguintes funções foram criadas para remover substituir os pronomes.\n","\n"," Já não é necessário voltar a correr."]},{"cell_type":"code","metadata":{"id":"z6dnGJEkZ1Sd"},"source":["import requests\n","import json\n","\n","with open('PW/allIn.json', 'r') as j:\n","    json_data = json.load(j)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c-vYrsg9q1Hm"},"source":["import csv\n","tsv_file = open(\"correto.tsv\")\n","read_tsv = csv.reader(tsv_file, delimiter=\"\\t\")\n","for row in read_tsv:\n","  aux = row[0].split(\"_\")\n","  try:\n","    json_data[aux[0]][aux[1]][\"utterance\"] = row[1]\n","  except:\n","    continue"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dO15UwZoaIAe"},"source":["json_data[\"78\"][\"6\"][\"utterance\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bDvBy-LrUL-G"},"source":["top10=[]\n","conversa =[]\n","pergunta=[]\n","tudo=[]\n","for conv in json_data:\n","  for utNumber in json_data[conv]:\n","      ut = json_data[conv][utNumber][\"utterance\"]\n","      tudo.append(ut)\n","      for i in json_data[conv][utNumber][\"top10\"]:\n","        tudo.append(i)\n","      pergunta.append(tudo)\n","      tudo = []\n","  conversa.append(pergunta)\n","  pergunta = []    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a4a63iUMrT6X"},"source":["Definir função para calcular o top10 respostas"]},{"cell_type":"code","metadata":{"id":"FHhASIyartPx"},"source":["def calcTop10 (transpost_ocurrences,scores,topicNumber,answerNumber):\n","    top10Scores=[]\n","    soma=0\n","    for sentence in transpost_ocurrences.toarray():\n","      for counter in range(0,len(sentence)):\n","        if sentence[counter]==1:\n","          soma += scores[counter]\n","      #mascara nos elementos que tem 0, só vai contar com os que têm 1\n","      #print(soma)\n","      top10Scores.append(soma)\n","      soma=0\n","    order=pd.DataFrame(top10Scores[1:11],columns=[\"Score\"])\n","    #order.insert(0,\"Id\",np.arange(0,10))\n","    order = order.sort_values(by='Score', ascending=False)\n","\n","    data=pd.DataFrame(conversa[topicNumber][answerNumber][1:11],columns=[\"TOP10RespostasOrdenadas\"])\n","    reordered=data.reindex(order.index)\n","    reordered.insert(1,\"Antigas\",conversa[topicNumber][answerNumber][1:11])\n","    print(reordered)\n","    return reordered"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fPIXatL9rzTt"},"source":["def addEntry(anteriores, atual):\n","  allEntidades = []\n","  for ant in anteriores:\n","        antArr = np.array(ant)\n","        entidades = []\n","        for principal in antArr:\n","          if principal[1][0]==1:\n","              entidades.append(principal[0])\n","        allEntidades.append(entidades)\n","\n","  for j in range(len(allEntidades)):\n","    for k in atual:\n","      if k[0] in allEntidades[j]:\n","        k[1]= np.insert(k[1],len(k[1]),1)\n","      else:\n","        k[1]= np.insert(k[1],len(k[1]),0)\n","  return atual"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xmz3owx3rdRj"},"source":["Definir a função que sumariza e retorna a resposta final"]},{"cell_type":"code","metadata":{"id":"QWfOW8eR5l0n"},"source":["#funcao que sumariza os 3 melhores textos\n","def sumarize (reordered):\n","  aux = reordered[\"TOP10RespostasOrdenadas\"].index\n","  toSumm = reordered[\"TOP10RespostasOrdenadas\"][aux[0]]+reordered[\"TOP10RespostasOrdenadas\"][aux[1]]+reordered[\"TOP10RespostasOrdenadas\"][aux[2]]\n","  summary_text = summarization(toSumm)[0]['summary_text']\n","  return summary_text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lkJHZqhxrjbM"},"source":["Definir a função que constrói a matriz e o grafo de entidades "]},{"cell_type":"code","metadata":{"id":"ep5miJ-yrqI-"},"source":["#retorna primeiro a matrix de entidades e depois a transposta e depois os scores\n","def create_EntityMatrix_Transpose_Pagerank(conteudo,threshold=3):\n","    #extrair os arrays que diz em que documento ocorre cada entidade\n","    arr = np.array(conteudo)\n","    aux = []\n","    for i in arr[:,1]:\n","      aux.append(i.tolist())\n","    #criar a matriz de entidades\n","    entitys = arr[:,0]\n","    matrix_entitys=csr_matrix(aux)\n","    array_ocurrences=matrix_entitys\n","    #transpose the matrix\n","    transpost_ocurrences=np.transpose(array_ocurrences)\n","    #Calculate the matrix of entitys\n","    mult = np.dot(array_ocurrences,transpost_ocurrences)\n","\n","    #Modificação professor\n","    graph=mult\n","    graph_cut = graph > threshold #(graph_cut fica com 0s e 1s)\n","    clean_graph = np.multiply(graph, graph_cut) #(a multiplicação vai fazer reset aos edges abaixo do threshold)\n","    clean_graph = np.multiply(graph, (graph > threshold))\n","    mult=clean_graph\n","    print(mult.todense())\n","    #aplicar o pagerank ao grafo\n","    entity_graph=csr_matrix(mult,dtype=float)\n","    scores=assistant.computePageRank(entity_graph)\n","\n","    #Dataframe to debug\n","    #data=pd.DataFrame(mult.todense(),columns=entitys)\n","    #data.insert(0,\"Entitys\",entitys)\n","    #print(data)\n","\n","    return entity_graph,transpost_ocurrences,entitys,scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZgdkzpHxQf0k"},"source":["#retorna primeiro a matrix de entidades e depois a transposta e depois os scores\n","def create_EntityMatrix_with1Q_Transpose_Pagerank(conteudo,anteriores,threshold=3):\n","    #extrair os arrays que diz em que documento ocorre cada entidade\n","    atual = np.array(conteudo)\n","    allEntidades =[]\n","\n","    atual = addEntry(anteriores,atual)\n","\n","    #Adicinar uma coluna\n","    aux = []\n","    for i in atual[:,1]:\n","      aux.append(i.tolist())\n","    #criar a matriz de entidades\n","    entitys = atual[:,0]\n","    matrix_entitys=csr_matrix(aux)\n","    array_ocurrences=matrix_entitys\n","    #transpose the matrix\n","    transpost_ocurrences=np.transpose(array_ocurrences)\n","    #Calculate the matrix of entitys\n","    mult = np.dot(array_ocurrences,transpost_ocurrences)\n","\n","    #Modificação professor\n","    graph=mult\n","    graph_cut = graph > threshold #(graph_cut fica com 0s e 1s)\n","    clean_graph = np.multiply(graph, graph_cut) #(a multiplicação vai fazer reset aos edges abaixo do threshold)\n","    clean_graph = np.multiply(graph, (graph > threshold))\n","    print(mult.todense())\n","    mult=clean_graph\n","    #aplicar o pagerank ao grafo\n","    entity_graph=csr_matrix(mult,dtype=float)\n","    scores= assistant.computePageRank(entity_graph)\n","\n","    #Dataframe to debug\n","    #data=pd.DataFrame(mult.todense(),columns=entitys)\n","    #data.insert(0,\"Entitys\",entitys)\n","    #print(data)\n","\n","    return entity_graph,transpost_ocurrences,entitys,scores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hZHC1YOWphoa"},"source":["Imagens"]},{"cell_type":"code","metadata":{"id":"eO66HBQ4xeZa"},"source":["def saveGraphImages(mult,scores,entitys,topicNumber,answerNumber,indice_modo):\n","  modos=[\"1-Normal\",\"2-Entidades_da_primeira\",\"3-Entidades_todas_perguntas\"\n","          ,\"4-Função_penalidade\"]\n","  topico=\"Topico_\"+str(topicNumber)\n","  modo=str(modos[indice_modo])\n","  pergunta=\"Pergunta_\"+str(answerNumber)\n","  dirName=\"PW/EntityGraphs/\"+modo+\"/\"+topico+\"/\"\n","  #dirName=\"PW/Isolado/\"+topico+\"/\"\n","  !mkdir -p $dirName\n","  image = svg_graph(mult, scores=np.log(scores), names = entitys,filename=dirName+pergunta)\n","  SVG(image)\n","  return image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YXBcqa4spo5N"},"source":["Main"]},{"cell_type":"code","metadata":{"id":"4AszBVuZpMkJ"},"source":["#Para todas as conversas\n","allx=[]\n","answersAll = []\n","allR = []\n","modo2Reordered=[]\n","aux = [\"31\",\"32\",\"33\",\"34\",\"37\",\"40\",\"49\",\"50\",\"54\",\"56\",\"58\",\"59\",\"61\",\"67\",\"68\",\"69\",\"75\",\"77\",\"78\",\"79\"]\n","#try:\n"," #   new_file=open(\"sumariosModo1.txt\",mode=\"w\",encoding=\"utf-8\")\n","#except IOError:\n"," #   print(\"File not found or path is incorrect\")\n","\n","for conv in range(0,len(allMatrixAllDataset)): #de 0 a 20 conversas\n","  \n","  #if conv not in (0,): # ESCOLHER OS TOPICOS AQUI\n","      #continue\n","\n","  modo0 = False # só normal\n","  modo1 = False #só 1º questão\n","  #modo2 = automático\n","  #modo3 = mudar o pickle \n","  treshold_1_num_entidades=3\n","\n","  conversa_topico=allMatrixAllDataset[conv]\n","  counterRespostas=0\n","  answersConv=[]\n","  reorderedConv = []\n","  for j in  range(0,len(conversa_topico)): #Para cada conteudo->set de (pergunta+respostas) num topico\n","\n","  #for conteudo in  conversa_topico: #Para cada conteudo->set de (pergunta+respostas) num topico\n","\n","    #1ºPasso: matrix de entidades e pagerank\n","    if j == 0:\n","      entity_graph, transpost_ocurrences, entitys, scores = create_EntityMatrix_Transpose_Pagerank(conversa_topico[j],treshold_1_num_entidades)\n","    else:\n","      if modo0:\n","        entity_graph, transpost_ocurrences, entitys, scores = create_EntityMatrix_Transpose_Pagerank(conversa_topico[j],treshold_1_num_entidades)\n","      if modo1:\n","        entity_graph, transpost_ocurrences, entitys, scores = create_EntityMatrix_with1Q_Transpose_Pagerank(conversa_topico[j], [conversa_topico[0]],treshold_1_num_entidades)\n","      else:\n","        entity_graph, transpost_ocurrences, entitys, scores = create_EntityMatrix_with1Q_Transpose_Pagerank(conversa_topico[j], conversa_topico[:j],treshold_1_num_entidades)\n","    print(\"entity_graph\")\n","    print(len(entity_graph.todense()))\n","    #1ºPasso (Opcional): criar imagens do grafo de entidades\n","    \n","    #image=saveGraphImages(entity_graph,scores,entitys,conv,counterRespostas,1)\n","\n","    #2ºPasso: calcular top10\n","    reordered = calcTop10(transpost_ocurrences,scores,conv,counterRespostas)\n","    allR.append(scores)\n","\n","    reorderedConv.append(reordered.index)\n","    #3ºPasso: calcular respostas sumarizada final\n","    #summary_text = sumarize(reordered)\n","    #new_file.write(str(aux[conv])+\"_\"+str(j)+\"\\n\")\n","    #new_file.write(summary_text+\"\\n\")\n","    #answersConv.append(summary_text)\n","    counterRespostas+=1\n","    \n","  modo2Reordered.append(reorderedConv)\n","  #answersAll.append(answersConv)\n","#new_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yzQjQYkC_dhK"},"source":["## Calcular pontuacoes"]},{"cell_type":"code","metadata":{"id":"gm-hKerD1DWF"},"source":["f = open(\"/content/PW/Extras/2019qrels.txt\", \"r\")\n","evaluated = {}\n","for x in f:\n","  y = x.split()\n","  try:\n","    kk = evaluated[y[0].split(\"_\")[0]]\n","  except:\n","    kk = {}\n","  try:\n","    vec = kk[y[0].split(\"_\")[1]]\n","  except:\n","    vec = {} \n","  vec[y[2]]=int(y[3])\n","  kk[y[0].split(\"_\")[1]] = vec\n","  evaluated[y[0].split(\"_\")[0]] = kk\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Verln1i92n8l"},"source":["import json\n","with open('/content/PW/Extras/allIn2.json', 'r') as j:\n","    json_data = json.load(j)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LP3mZeSk-Gww"},"source":["def soma (allReordered,json_data,evaluated):\n","  sum = 0\n","  for k in range(3):\n","      #pos = allReordered[counter][int(j)-1][k]\n","      pos = allReordered[k]\n","      #cod = json_data[i][j][\"top10\"][pos]\n","      cod = json_data[pos]\n","      try:\n","        #sum = sum +  evaluated[i][j][cod]\n","        sum = sum +  evaluated[cod]\n","      except:\n","        #return -1\n","        continue\n","  return sum\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C9bVyP0GYHYt"},"source":["def calcMax(arr,soma):\n","    res = \"\"\n","    maxi = max(arr)\n","    if maxi == 0:\n","      return \"0\",soma\n","    if maxi == arr[0]:\n","      res = res + \"modo0_\"\n","      soma[0] = soma[0]+1\n","    if maxi == arr[1]:\n","      res = res + \"modo1_\"\n","      soma[1] = soma[1]+1\n","    if maxi == arr[2]:\n","      res = res + \"modo2_\"\n","      soma[2] = soma[2]+1\n","    if maxi == arr[3]:\n","      res = res + \"modo3\"\n","      soma[3] = soma[3]+1\n","    return res, soma"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ueeXAUfRBPjp"},"source":["import csv  \n","\n","header = ['modo0','modo1', 'modo2', 'modo3']\n","#header = ['pergunta', 'modo0','modo1', 'modo2', 'modo3']\n","somas = [0] * 4\n","with open('resultados3.csv', 'w', encoding='UTF8') as f:\n","    writer = csv.writer(f)\n","    # write the header\n","    writer.writerow(header)\n","    counter = 0\n","    for i in json_data:\n","      print(i)\n","      somas = [0] * 4\n","      for j in json_data[i]:\n","        print(j)\n","        try:\n","          modo0 = soma(modo0Reordered[counter][int(j)-1],json_data[i][j][\"top10\"],evaluated[i][j])\n","          modo1 = soma(modo1Reordered[counter][int(j)-1],json_data[i][j][\"top10\"],evaluated[i][j])\n","          modo2 = soma(modo2Reordered[counter][int(j)-1],json_data[i][j][\"top10\"],evaluated[i][j])\n","          modo3 = soma(modo3Reordered[counter][int(j)-1],json_data[i][j][\"top10\"],evaluated[i][j])\n","          comp = [modo0,modo1,modo2,modo3,modo4]\n","          if -1 not in comp:\n","            maxIs,somas = calcMax(comp,somas)\n","            print(comp)\n","            data = [i+'_'+j,modo0,modo1,modo2,modo3,maxIs]\n","            #writer.writerow(data)\n","          #else:\n","            #print(\"hi\")   \n","        except:\n","          print(\"here\")\n","          continue\n","      #writer.writerow(somas)\n","      counter = counter + 1\n","    # write the data\n","print(somas)\n","f.close()\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1cqWoIog5bUt"},"source":["## 4º Passo"]},{"cell_type":"markdown","metadata":{"id":"P5QhzrNI5guK"},"source":["### Guardar o Sumarization num txt"]},{"cell_type":"markdown","metadata":{"id":"4l9dCJYqEmOg"},"source":["Guardar num ficheiro txt as respostas"]},{"cell_type":"code","metadata":{"id":"n1VoIWncEq3V"},"source":["!mkdir -p PW/SumarioTxt\n","count=0\n","with open(\"PW/SumarioTxt/AllSumarized.txt\", \"w\") as txt_file:\n","    for topic in answersAll:\n","        txt_file.write(\"---------------------\" + \"\\n\")\n","        txt_file.write(\"Topico \" + str(count) + \"\\n\") # works with any number of elements in a line\n","        for ans in topic:\n","          txt_file.write(\"\".join(ans) + \"\\n\") # works with any number of elements in a line\n","    count+=1\n","    txt_file.write(\"---------------------\" + \"\\n\")"],"execution_count":null,"outputs":[]}]}